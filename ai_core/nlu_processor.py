# ai_core/nlu_processor.py
import json
import re
import logging
import unicodedata
from typing import Dict, Any
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import BaseOutputParser

from .llm_chain import GeminiLLM  # Wrapper LLM tu·ª≥ ch·ªânh

# ========================
# Logging setup
# ========================
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


# ========================
# Output Parser
# ========================
class NLUOutputParser(BaseOutputParser):
    """Parser an to√†n cho output t·ª´ LLM (JSON -> Dict)"""

    def parse(self, text: str) -> Dict[str, Any]:
        try:
            logger.debug(f"üîπ Raw LLM output: {text}")
            # T√¨m JSON trong text
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                logger.debug(f"‚úÖ Parsed JSON: {parsed}")
                return parsed
            else:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y JSON trong output")
                return {}
        except Exception as e:
            logger.error(f"‚ùå Parse l·ªói: {e}")
            return {}

    def get_format_instructions(self) -> str:
        """H∆∞·ªõng d·∫´n format JSON cho LLM (c√≥ th·ªÉ d√πng trong prompt)."""
        return "Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng JSON h·ª£p l·ªá."

# ========================
# Context Memory (simple)
# ========================
class ContextMemory:
    def __init__(self):
        self.last_intent = None
        self.last_entities = {}

    def update(self, intent: str, entities: Dict[str, Any]):
        if intent and intent != "unknown":
            self.last_intent = intent
        if entities:
            self.last_entities = entities

    def get_context(self) -> Dict[str, Any]:
        return {
            "last_intent": self.last_intent,
            "last_entities": self.last_entities,
        }
    
# ========================
# Helpers (NEW)
# ========================
def _strip_accents(s: str) -> str:
    """B·ªè d·∫•u ti·∫øng Vi·ªát ƒë·ªÉ so kh·ªõp keyword d·ªÖ h∆°n."""
    if not isinstance(s, str):
        return ""
    return "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")

def _contains_any(haystack: str, needles) -> bool:
    return any(n in haystack for n in needles)

# C√°c t·ª´ g·ª£i √Ω c√¢u h·ªèi KH√îNG ph·∫£i hotline (ƒë·ªÉ kh√¥ng √©p v·ªÅ hotline)
QUESTION_TRIGGERS = [
    "·ªü ƒë√¢u", "o dau", "l√† g√¨", "la gi", "gi·ªõi thi·ªáu", "gioi thieu",
    "th√¥ng tin", "thong tin", "bao nhi√™u", "bao nhieu", "v√¨ sao", "vi sao", "?"
]

# T·ª´ kho√° hotline
HOTLINE_KEYWORDS = [
    "hotline", "s·ªë ƒëi·ªán tho·∫°i", "so dien thoai", "ƒëi·ªán tho·∫°i", "dien thoai",
    "ƒë∆∞·ªùng d√¢y n√≥ng", "duong day nong", "g·ªçi", "goi", "li√™n h·ªá", "lien he"
]

# Danh s√°ch ƒë·ªãa danh ph·ªï bi·∫øn (c√≥ c·∫£ c√≥ d·∫•u & kh√¥ng d·∫•u, ƒë·ªÉ match nhanh)
LOCATION_TOKENS = [
    "qu·∫£ng b√¨nh","quang binh","qb",
    "qu·∫£ng tr·ªã","quang tri","qt",
    "th·ª´a thi√™n hu·∫ø","thua thien hue","hu·∫ø","hue","tth",
    "ƒë√† n·∫µng","da nang","dn",
    "qu·∫£ng nam","quang nam","qn",
    "ngh·ªá an","nghe an","na",
    "h√† tƒ©nh","ha tinh","ht",
    "thanh h√≥a","thanh hoa","th",
    # c√≥ th·ªÉ b·ªï sung th√™m...
]
# ========================
# NLU Processor
# ========================
class NLUProcessor:
    def __init__(self, llm=None, memory_manager=None):
        """
        llm: object LLM, n·∫øu None s·∫Ω t·ª± kh·ªüi t·∫°o GeminiLLM m·∫∑c ƒë·ªãnh
        memory_manager: ƒë·ªÉ truy c·∫≠p last_intent, last_question, chat_history
        """
        self.llm = llm or GeminiLLM()
        self.memory_manager = memory_manager
        self.memory = ContextMemory()  # ‚úÖ th√™m b·ªô nh·ªõ ng·ªØ c·∫£nh
        self.setup_intent_detection()
        self.setup_entity_extraction()

    # ----------------------------
    # Intent Detection
    # ----------------------------
    def setup_intent_detection(self):
        intent_template = """
        Ph√¢n t√≠ch c√¢u h·ªèi sau v√† x√°c ƒë·ªãnh √Ω ƒë·ªãnh (intent) c·ªßa ng∆∞·ªùi d√πng.
        C√°c intent c√≥ th·ªÉ l√†:
        - definition: h·ªèi v·ªÅ ƒë·ªãnh nghƒ©a, kh√°i ni·ªám
        - safety_advice: h·ªèi v·ªÅ h∆∞·ªõng d·∫´n an to√†n
        - location_info: h·ªèi v·ªÅ th√¥ng tin ƒë·ªãa ƒëi·ªÉm (v√≠ d·ª•: "Qu·∫£ng Tr·ªã c√≥ g√¨ ƒë·∫∑c bi·ªát?")
        - report_uxo: b√°o c√°o v·∫≠t n·ªï
        - ask_hotline: h·ªèi s·ªë hotline (v√≠ d·ª•: "s·ªë ƒëi·ªán tho·∫°i Qu·∫£ng Tr·ªã", "hotline ·ªü ƒë√¢u?")
        - general: c√¢u h·ªèi chung kh√°c

        PH√ÇN BI·ªÜT QUAN TR·ªåNG:
        - "Qu·∫£ng Tr·ªã" ‚Üí location_info (n·∫øu ch·ªâ l√† t√™n ƒë·ªãa ƒëi·ªÉm kh√¥ng ng·ªØ c·∫£nh)
        - "s·ªë ƒëi·ªán tho·∫°i Qu·∫£ng Tr·ªã" ‚Üí ask_hotline
        - "hotline Qu·∫£ng Tr·ªã" ‚Üí ask_hotline

        C√¢u h·ªèi: {question}
        Ng√¥n ng·ªØ: {language}

        Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng JSON v·ªõi c·∫•u tr√∫c:
        {{
            "intent": "t√™n_intent",
            "confidence": s·ªë_th·∫≠p_ph√¢n_t·ª´_0_ƒë·∫øn_1
        }}
        """

        self.intent_prompt = PromptTemplate(
            template=intent_template,
            input_variables=["question", "language"],
        )

        self.intent_chain = LLMChain(
            llm=self.llm,
            prompt=self.intent_prompt,
            output_parser=NLUOutputParser(),
            output_key="answer"
        )

    # ----------------------------
    # Entity Extraction
    # ----------------------------
    def setup_entity_extraction(self):
        entity_template = """
        Tr√≠ch xu·∫•t th·ª±c th·ªÉ (entities) t·ª´ c√¢u h·ªèi sau:
        C√¢u h·ªèi: {question}
        Ng√¥n ng·ªØ: {language}

        C√°c lo·∫°i th·ª±c th·ªÉ c·∫ßn tr√≠ch xu·∫•t:
        - location: ƒë·ªãa ƒëi·ªÉm, t·ªânh th√†nh
        - uxo_type: lo·∫°i v·∫≠t n·ªï (bom, m√¨n, l·ª±u ƒë·∫°n, etc.)
        - action: h√†nh ƒë·ªông

        Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng JSON v·ªõi c·∫•u tr√∫c:
        {{
            "entities": {{
                "location": [],
                "uxo_type": [],
                "action": []
            }}
        }}
        """

        self.entity_prompt = PromptTemplate(
            template=entity_template,
            input_variables=["question", "language"],
        )

        self.entity_chain = LLMChain(
            llm=self.llm,
            prompt=self.entity_prompt,
            output_parser=NLUOutputParser(),
            output_key="answer"
        )

    # ----------------------------
    # API: Detect Intent
    # ----------------------------

    def _get_last_assistant_message(self, session_id: str) -> str:
        """üîπ NEW: l·∫•y tin nh·∫Øn assistant g·∫ßn nh·∫•t (n·∫øu c√≥) ƒë·ªÉ bi·∫øt c√≥ ƒëang h·ªèi xin ƒë·ªãa danh hotline kh√¥ng."""
        if not self.memory_manager:
            return ""
        try:
            msgs = self.memory_manager.get_messages(session_id)
            for m in reversed(msgs):
                # Trong memory_manager c·ªßa b·∫°n, msg.type == "human"/"ai"
                if hasattr(m, "type") and m.type != "human":
                    return getattr(m, "content", "") or ""
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c last assistant message: {e}")
        return ""

    def detect_intent(self, question: str, language: str = "vi", session_id: str = "default") -> Dict[str, Any]:
        try:
            raw = self.intent_chain.invoke({"question": question, "language": language})
            output_text = raw["answer"] if isinstance(raw, dict) and "answer" in raw else raw
            if isinstance(output_text, dict):
                output_text = json.dumps(output_text, ensure_ascii=False)
            parsed = self.intent_chain.output_parser.parse(output_text)

            intent = parsed.get("intent", "unknown")
            confidence = float(parsed.get("confidence", 0.0))
            enriched_text = None

            # üîπ Context-based refinement (STRICT)
            last_intent, last_question = "", ""
            awaiting_hotline_location = False

            if self.memory_manager:
                last_intent = self.memory_manager.get_last_intent(session_id)
                last_question = self.memory_manager.get_last_question(session_id)

                # Nh·∫≠n di·ªán xem bot c√≥ ƒëang h·ªèi ng∆∞·ªùi d√πng "hotline ·ªü khu v·ª±c n√†o?" kh√¥ng
                last_bot = self._get_last_assistant_message(session_id)
                last_bot_lc = _strip_accents((last_bot or "").lower())
                if "hotline o khu vuc nao" in last_bot_lc or "ban muon hoi so hotline" in last_bot_lc:
                    awaiting_hotline_location = True

            # Chu·∫©n ho√° text ƒë·ªÉ so kh·ªõp
            t_lc = question.strip().lower()
            t_ascii = _strip_accents(t_lc)
            words = t_ascii.split()
            is_short = len(words) <= 4

            has_question_word = _contains_any(t_lc, QUESTION_TRIGGERS) or _contains_any(t_ascii, QUESTION_TRIGGERS)
            has_hotline_kw = _contains_any(t_lc, HOTLINE_KEYWORDS) or _contains_any(t_ascii, HOTLINE_KEYWORDS)
            has_location = any(tok in t_lc or tok in t_ascii for tok in LOCATION_TOKENS)

            # üéØ QUY T·∫ÆC M·ªöI (fix bug b·∫°n g·∫∑p):
            # Ch·ªâ √©p v·ªÅ ask_hotline khi:
            #  - ƒêANG CH·ªú ƒë·ªãa danh cho hotline (bot v·ª´a h·ªèi khu v·ª±c) HO·∫∂C last_intent l√† ask_hotline
            #  - C√¢u r·∫•t ng·∫Øn & CH·ªà l√† ƒë·ªãa danh (kh√¥ng c√≥ t·ª´ nghi v·∫•n/mi√™u t·∫£)
            #  - Kh√¥ng ch·ª©a t·ª´ kho√° hotline (v√¨ khi ƒë√≥ intent ƒë√£ l√† ask_hotline t·ª± nhi√™n)
            if (
                has_location
                and is_short
                and not has_question_word
                and not has_hotline_kw
                and (awaiting_hotline_location or last_intent == "ask_hotline")
            ):
                logger.debug("‚ö° Follow-up hotline h·ª£p l·ªá: √©p intent = ask_hotline")
                intent = "ask_hotline"
                enriched_text = f"{last_question} {question}" if last_question else f"hotline {question}"
            else:
                # N·∫øu c√¢u h·ªèi c√≥ t·ª´ nghi v·∫•n nh∆∞ '·ªü ƒë√¢u', 'l√† g√¨'... th√¨ KH√îNG √©p hotline
                logger.debug("‚ÑπÔ∏è Kh√¥ng √©p intent v·ªÅ hotline (gi·ªØ theo LLM ho·∫∑c suy lu·∫≠n th∆∞·ªùng).")

            # Tr∆∞·ªùng h·ª£p c√¢u c·ª±c ng·∫Øn (<=2 t·ª´) ‚Üí enrich text ƒë·ªÉ RAG hi·ªÉu h∆°n
            if not enriched_text and last_question and len(words) <= 2:
                enriched_text = f"{last_question} {question}"
                logger.debug(f"üß© Enriched text (c√¢u c·ª±c ng·∫Øn): {enriched_text}")

            return {
                "intent": intent,
                "confidence": confidence,
                "last_intent": last_intent,
                "last_question": last_question,
                "awaiting_hotline_location": awaiting_hotline_location,
                "enriched_text": enriched_text
            }

        except Exception as e:
            logger.error(f"‚ùå Intent detection l·ªói: {e}")
            return {
                "intent": "unknown",
                "confidence": 0.0,
                "last_intent": "",
                "last_question": "",
                "awaiting_hotline_location": False,
                "enriched_text": None
            }
    # ----------------------------
    # API: Extract Entities
    # ----------------------------
    def extract_entities(self, question: str, language: str = "vi") -> Dict[str, Any]:
        try:
            result = self.entity_chain.invoke({"question": question, "language": language})
            output_text = result["answer"] if isinstance(result, dict) and "answer" in result else result
            if isinstance(output_text, dict):
                output_text = json.dumps(output_text, ensure_ascii=False)
            parsed = self.entity_chain.output_parser.parse(output_text)
            return {
                "entities": parsed.get(
                    "entities", {"location": [], "uxo_type": [], "action": []}
                )
            }
        except Exception as e:
            logger.error(f"‚ùå Entity extraction l·ªói: {e}")
            return {"entities": {"location": [], "uxo_type": [], "action": []}}
    # ----------------------------
    # API: Full NLU Pipeline
    # ----------------------------
    def process_nlu(self, question: str, language: str = "vi", session_id: str = "default") -> Dict[str, Any]:
        intent_result = self.detect_intent(question, language, session_id)
        entity_result = self.extract_entities(question, language)

        merged = {
            "intent": intent_result["intent"],
            "confidence": intent_result["confidence"],
            "entities": entity_result["entities"],
            "last_intent": intent_result.get("last_intent", ""),
            "last_question": intent_result.get("last_question", ""),
            "awaiting_hotline_location": intent_result.get("awaiting_hotline_location", False),
            "enriched_text": intent_result.get("enriched_text")
        }
        logger.debug(f"‚úÖ Final NLU output: {merged}")
        return merged
